{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import pickle \n",
    "from politenessr import Politenessr\n",
    "from transformers import XLMRobertaTokenizer\n",
    "from thefuzz import fuzz\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pickle(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        pickle_file = pickle.load(file)\n",
    "    return pickle_file\n",
    "\n",
    "en_dialog = read_pickle(\"dialog_act_data/english_dialog_acts_updated.p\")\n",
    "es_dialog = read_pickle(\"dialog_act_data/spanish_dialog_acts_updated.p\")\n",
    "ja_dialog = read_pickle(\"dialog_act_data/japanese_dialog_acts_updated.p\")\n",
    "zh_dialog = read_pickle(\"dialog_act_data/chinese_dialog_acts_updated.p\")\n",
    "\n",
    "en_shap = read_pickle(\"../shapley/xlm_shap_values/english.p\")\n",
    "es_shap = read_pickle(\"../shapley/xlm_shap_values/spanish.p\")\n",
    "ja_shap = read_pickle(\"../shapley/xlm_shap_values/japanese.p\")\n",
    "zh_shap = read_pickle(\"../shapley/xlm_shap_values/chinese.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_sentences(df):\n",
    "    total_len = 0\n",
    "    for u in df[\"Translated Sentences\"]:\n",
    "        total_len += len(u)\n",
    "    return total_len\n",
    "\n",
    "print(\"num_sent_en = \", get_num_sentences(en_dialog))\n",
    "print(\"num_sent_es = \", get_num_sentences(es_dialog))\n",
    "print(\"num_sent_ja = \", get_num_sentences(ja_dialog))\n",
    "print(\"num_sent_zh = \", get_num_sentences(zh_dialog))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_level_shapley_values(shap_pickle, tokenizer):\n",
    "    token_level_shapley_values = []\n",
    "    utterances = []\n",
    "    shap_data = shap_pickle.data\n",
    "    shap_values = shap_pickle.values\n",
    "    shap_base = shap_pickle.base_values\n",
    "    for idx in tqdm(range(len(shap_data))):\n",
    "        tokens = shap_data[idx]\n",
    "        values = shap_values[idx]\n",
    "        base = shap_base[idx]\n",
    "\n",
    "        # divide base amongst all tokens\n",
    "        base_add = base/len(tokens)\n",
    "        values = [v + base_add for v in values]\n",
    "        \n",
    "        utterance_string = tokenizer.convert_tokens_to_string(tokens)\n",
    "        utterances.append(utterance_string)\n",
    "        token_level_shapley_values.append([tokens, values, base])\n",
    "    return utterances, token_level_shapley_values\n",
    "\n",
    "def process_tokens(token_list):\n",
    "    #remove all spaces and punctuation from token that are added by tokenizer\n",
    "    to_replace = [\" \", \"_\", \"▁\", \".\", \",\", \"\\n\", \")\", \"(\", \"[\", \"]\"]\n",
    "    for r in to_replace:\n",
    "        token_list = [x.replace(r, \"\") for x in token_list]\n",
    "    #make all lowercase\n",
    "    token_list = [str(x).lower().strip() for x in token_list]\n",
    "    #remove all empty strings\n",
    "    return token_list\n",
    "\n",
    "def get_start_end_idxs(shap_tokens, match_tokens, fuzz_threshold):\n",
    "    start_idx = -1\n",
    "    end_idx = -1\n",
    "\n",
    "    max_fuzz = fuzz_threshold\n",
    "\n",
    "    processed_shap_tokens = process_tokens(shap_tokens)\n",
    "    processed_match_tokens = process_tokens(match_tokens)\n",
    "\n",
    "    match_sent = tokenizer.convert_tokens_to_string(match_tokens)\n",
    "\n",
    "    for j in range(len(processed_shap_tokens)):\n",
    "        if(processed_shap_tokens[j] == processed_match_tokens[0]):\n",
    "            sent_1 = tokenizer.convert_tokens_to_string(shap_tokens[j:j+len(match_tokens)])\n",
    "            sent_2 = tokenizer.convert_tokens_to_string(shap_tokens[j:min(j+len(match_tokens)+1, len(shap_tokens))])\n",
    "            sent_3 = tokenizer.convert_tokens_to_string(shap_tokens[j:min(j+len(match_tokens)+2, len(shap_tokens))])\n",
    "            fuzz1 = fuzz.ratio(sent_1, match_sent)\n",
    "            fuzz2 = fuzz.ratio(sent_2, match_sent)\n",
    "            fuzz3 = fuzz.ratio(sent_3, match_sent)\n",
    "            if(fuzz1 > max_fuzz):\n",
    "                start_idx = j\n",
    "                end_idx = j+len(match_tokens)\n",
    "                max_fuzz = fuzz1\n",
    "            if(fuzz2 > max_fuzz):\n",
    "                start_idx = j\n",
    "                end_idx = j+len(match_tokens)+1\n",
    "                max_fuzz = fuzz2\n",
    "            if(fuzz3 > max_fuzz):\n",
    "                start_idx = j\n",
    "                end_idx = j+len(match_tokens)+2\n",
    "                max_fuzz = fuzz3\n",
    "    return start_idx, end_idx\n",
    "\n",
    "def string_to_list(string):\n",
    "    string = string.replace(\"[\", \"\")\n",
    "    string = string.replace(\"]\", \"\")\n",
    "    list_items = string.split(\"\\\",\")\n",
    "    final_items = []\n",
    "    for item in list_items:\n",
    "        split_items = item.split(\"\\',\")\n",
    "        for i in range(len(split_items)):\n",
    "            final_items.append(split_items[i])\n",
    "    for i in range(len(final_items)):\n",
    "        final_items[i] = final_items[i].strip()\n",
    "        final_items[i] = final_items[i][1:]\n",
    "    return final_items\n",
    "\n",
    "def process_dialog_acts(dialog_labels):\n",
    "    for i in range(len(dialog_labels)):\n",
    "        dialog_labels[i] = str(dialog_labels[i])\n",
    "        dialog_labels[i] = dialog_labels[i].replace(\"[\", \"\")\n",
    "        dialog_labels[i] = dialog_labels[i].replace(\"]\", \"\")\n",
    "        dialog_labels[i] = dialog_labels[i].replace(\"'\", \"\")\n",
    "        dialog_labels[i] = dialog_labels[i].replace(\",\", \"\")\n",
    "        dialog_labels[i] = dialog_labels[i].replace(\"(\", \"\")\n",
    "        dialog_labels[i] = dialog_labels[i].replace(\")\", \"\")\n",
    "        dialog_labels[i] = dialog_labels[i].strip()\n",
    "    return dialog_labels\n",
    "\n",
    "\n",
    "def get_sentence_shapley(shapley_values, dialog_acts, fuzz_threshold=70, return_sentences=False):\n",
    "    _, shap_object = get_token_level_shapley_values(shapley_values, tokenizer)\n",
    "    no_match_count = 0\n",
    "    shap_sentence_vals = []\n",
    "    dialog_sentence_labels = []\n",
    "    raw_sentences = []\n",
    "    assert(len(shapley_values) == len(dialog_acts))\n",
    "    for i in range(len(shapley_values)):\n",
    "        sentences = dialog_acts[\"Sentences\"][i]\n",
    "        split_sentences = string_to_list(sentences)\n",
    "        \n",
    "        dialog_labels = dialog_acts[\"Dialog Acts\"][i]\n",
    "        dialog_labels = process_dialog_acts(dialog_labels)\n",
    "        try:\n",
    "            assert(len(split_sentences) == len(dialog_labels))\n",
    "        except(AssertionError):\n",
    "            continue\n",
    "        \n",
    "        local_sentence_vals = []\n",
    "        shap_tokens = shap_object[i][0]\n",
    "        for sent in split_sentences:\n",
    "            match_tokens = tokenizer.tokenize(sent)\n",
    "            start_idx, end_idx = get_start_end_idxs(shap_tokens, match_tokens, fuzz_threshold)\n",
    "            if(start_idx == -1 or end_idx == -1):\n",
    "                start_idx, end_idx = get_start_end_idxs(shap_tokens, match_tokens[1:], fuzz_threshold)\n",
    "            if(start_idx == -1 or end_idx == -1):\n",
    "                start_idx, end_idx = get_start_end_idxs(shap_tokens, match_tokens[2:], fuzz_threshold)\n",
    "            if(start_idx == -1 or end_idx == -1):\n",
    "                no_match_count += 1\n",
    "            total_value = np.sum(shap_object[i][1][start_idx:end_idx])\n",
    "            local_sentence_vals.append(total_value)\n",
    "        shap_sentence_vals.append(local_sentence_vals)\n",
    "        dialog_sentence_labels.append(dialog_labels)\n",
    "        raw_sentences.append(split_sentences)\n",
    "\n",
    "    print(\"No match count: %d\" % no_match_count)\n",
    "    if(return_sentences):\n",
    "        return shap_sentence_vals, dialog_sentence_labels, raw_sentences\n",
    "    return shap_sentence_vals, dialog_sentence_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_sent_en, dialog_labels_en = get_sentence_shapley(en_shap, en_dialog, 80)\n",
    "shap_sent_es, dialog_labels_es = get_sentence_shapley(es_shap, es_dialog, 80)\n",
    "shap_sent_ja, dialog_labels_ja = get_sentence_shapley(ja_shap, ja_dialog, 60)\n",
    "shap_sent_zh, dialog_labels_zh = get_sentence_shapley(zh_shap, zh_dialog, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialog_act_labels = {\n",
    "    \"s\": \"Statement\",\n",
    "    \"sd\": \"Declarative Statement\",\n",
    "    \"sv\": \"Opinion Statement\",\n",
    "    \"oo\": \"Open-option\",\n",
    "    \"qy\": \"Yes/No Question\",\n",
    "    \"qw\": \"\\\"Wh-\\\" Question\",\n",
    "    \"qo\": \"Open Question\",\n",
    "    \"qr\": \"Or-Question\",\n",
    "    \"qrr\": \"Or-Clause\",\n",
    "    \"qh\": \"Rhetorical Question\",\n",
    "    \"^d\": \"Declarative Question\",\n",
    "    \"qy^d\": \"Declarative Yes/No Question\",\n",
    "    \"^g\": \"Tag Question\",\n",
    "    \"ad\": \"Directive Action\",\n",
    "    \"co\": \"Offer\",\n",
    "    \"cc\": \"Commit\",\n",
    "    \"fp\": \"Conventional Opening\",\n",
    "    \"fc\": \"Conventional Closing\",\n",
    "    \"fx\": \"Explicit-performative\",\n",
    "    \"fe\": \"Exclamation\",\n",
    "    \"fo\": \"Other-forward-function\",\n",
    "    \"ft\": \"Forward-Function: Thanking\",\n",
    "    \"fw\": \"Forward-Function: You're-Welcome\",\n",
    "    \"fa\": \"Forward-Function: Apology\",\n",
    "    \"aa\": \"Acceptance\",\n",
    "    \"aap\": \"Accept-part\",\n",
    "    \"am\": \"Maybe\",\n",
    "    \"arp\": \"Reject-part\",\n",
    "    \"ar\": \"Reject\",\n",
    "    \"^h\": \"Hold before answer/agreement\",\n",
    "    \"br\": \"Signal-non-understanding\",\n",
    "    \"br^m\": \"Signal-non-understanding\",\n",
    "    \"b\": \"Acknowledge\",\n",
    "    \"bh\": \"Acknowledge\",\n",
    "    \"bk\": \"Acknowledge-answer\",\n",
    "    \"^m\": \"Repeat-phrase\",\n",
    "    \"^2\": \"Completion\",\n",
    "    \"bf\": \"Summarize/Reformulate\",\n",
    "    \"ba\": \"Appreciation\",\n",
    "    \"by\": \"Sympathy\",\n",
    "    \"bd\": \"Downplayer\",\n",
    "    \"bc\": \"Correct-misspeaking\",\n",
    "    \"ny\":  \"Yes answers\",\n",
    "    \"nn\":  \"No answers\",\n",
    "    \"na\":  \"Affirmative non-yes answers\",\n",
    "    \"nd\":  \"Dispreferred answers\",\n",
    "    \"ng\":  \"Negative non-No Answer\",\n",
    "    \"no\":  \"Other answers\",\n",
    "    \"sd^e\": \"Statement expanding y/n answer\",\n",
    "    \"sv^e\": \"Statement expanding y/n answer\",\n",
    "    \"^e\":  \"Expansions of y/n answers\",\n",
    "    \"nn^e\": \"No plus expansion\",\n",
    "    \"ny^e\": \"Yes plus expansion\",\n",
    "    \"^q\":  \"Quoted material\",\n",
    "}\n",
    "\n",
    "def relabel_dict(dictionary):\n",
    "    #rename keys to be more readable\n",
    "    new_dict = {}\n",
    "    for key in dictionary.keys():     \n",
    "        if(key == \"Summarize/Reformulate\"): key_new = \"Summarize/\\nReformulate\"\n",
    "        elif(key == \"Declarative Yes/No Question\"): key_new = \"Declarative\\nYes/No Question\"\n",
    "        else: key_new = key.replace(\" \", \"\\n\") \n",
    "        new_dict[key_new] = dictionary[key]\n",
    "    return new_dict\n",
    "\n",
    "def get_dialog_mapping(shap_sent, dialog_labels):\n",
    "    assert(len(shap_sent) == len(dialog_labels))\n",
    "    shap_dialog_mapping = {}\n",
    "    shap_sent_flatten = np.concatenate(shap_sent)\n",
    "    dialog_labels_flatten = np.concatenate(dialog_labels)\n",
    "    for i in range(len(shap_sent_flatten)):\n",
    "        dialog_acts_split = dialog_labels_flatten[i].split(\" \")\n",
    "        for split_act in dialog_acts_split:\n",
    "            split_act = split_act.strip()\n",
    "            try:\n",
    "                shap_dialog_mapping[split_act].append(shap_sent_flatten[i])\n",
    "            except(KeyError):\n",
    "                shap_dialog_mapping[split_act] = [shap_sent_flatten[i]]\n",
    "    return shap_dialog_mapping\n",
    "\n",
    "def average_shap_vals(mapping):\n",
    "    mapping_aggregated = {}\n",
    "    mapping_counts = {}\n",
    "    for category in mapping:\n",
    "        if(len(mapping[category]) < 1):\n",
    "            continue\n",
    "        mapping_aggregated[category] = np.mean(mapping[category])\n",
    "        mapping_counts[category] = len(mapping[category])\n",
    "    return mapping_aggregated, mapping_counts\n",
    "\n",
    "mapping_en = get_dialog_mapping(shap_sent_en, dialog_labels_en)\n",
    "mapping_es = get_dialog_mapping(shap_sent_es, dialog_labels_es)\n",
    "mapping_ja = get_dialog_mapping(shap_sent_ja, dialog_labels_ja)\n",
    "mapping_zh = get_dialog_mapping(shap_sent_zh, dialog_labels_zh)\n",
    "\n",
    "en_avg_raw, en_counts_raw = average_shap_vals(mapping_en)\n",
    "es_avg_raw, es_counts_raw = average_shap_vals(mapping_es)\n",
    "ja_avg_raw, ja_counts_raw = average_shap_vals(mapping_ja)\n",
    "zh_avg_raw, zh_counts_raw = average_shap_vals(mapping_zh)\n",
    "\n",
    "THRESH = 20\n",
    "#get all dialog acts that have count > THRESH in all languages\n",
    "all_dialog_acts = []\n",
    "for category in en_counts_raw:\n",
    "    if(category in es_counts_raw and category in ja_counts_raw and category in zh_counts_raw):\n",
    "        if(category not in dialog_act_labels): continue\n",
    "        if(en_counts_raw[category] > THRESH and es_counts_raw[category] > THRESH and ja_counts_raw[category] > THRESH and zh_counts_raw[category] > THRESH):\n",
    "            all_dialog_acts.append(category)\n",
    "\n",
    "#trim avg and count dicts to only include dialog acts that have count > THRESH in all languages\n",
    "en_avg = {dialog_act_labels[k]: v for k, v in en_avg_raw.items() if k in all_dialog_acts}\n",
    "es_avg = {dialog_act_labels[k]: v for k, v in es_avg_raw.items() if k in all_dialog_acts}\n",
    "ja_avg = {dialog_act_labels[k]: v for k, v in ja_avg_raw.items() if k in all_dialog_acts}\n",
    "zh_avg = {dialog_act_labels[k]: v for k, v in zh_avg_raw.items() if k in all_dialog_acts}\n",
    "\n",
    "en_counts = {dialog_act_labels[k]: v for k, v in en_counts_raw.items() if k in all_dialog_acts}\n",
    "es_counts = {dialog_act_labels[k]: v for k, v in es_counts_raw.items() if k in all_dialog_acts}\n",
    "ja_counts = {dialog_act_labels[k]: v for k, v in ja_counts_raw.items() if k in all_dialog_acts}\n",
    "zh_counts = {dialog_act_labels[k]: v for k, v in zh_counts_raw.items() if k in all_dialog_acts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make bar graphs for each dialog act category\n",
    "num_sent_en =  15624\n",
    "num_sent_es =  14952\n",
    "num_sent_ja =  13057\n",
    "num_sent_zh =  13656\n",
    "\n",
    "value_to_label_map = {}\n",
    "for key in en_avg.keys():\n",
    "    to_iterate = [en_avg, es_avg, ja_avg, zh_avg]\n",
    "    counts_to_iterate = [en_counts, es_counts, ja_counts, zh_counts]\n",
    "    sen_length_to_iterate = [num_sent_en, num_sent_es, num_sent_ja, num_sent_zh]\n",
    "    for idx, _ in enumerate(to_iterate):\n",
    "        val = to_iterate[idx][key]\n",
    "        count = counts_to_iterate[idx][key]\n",
    "        num_sent = sen_length_to_iterate[idx]\n",
    "        count_percent = count/num_sent * 100\n",
    "        #format val to string with 3 sig figs\n",
    "        val_string = \"{:.3f}\".format(val)\n",
    "        count_string = \"{:.1f}%\".format(count_percent)\n",
    "        label = val_string + \"  (\" + count_string + \")\"\n",
    "        value_to_label_map[val] = label\n",
    "\n",
    "en_avg_relabeled = relabel_dict(en_avg)\n",
    "es_avg_relabeled = relabel_dict(es_avg)\n",
    "ja_avg_relabeled = relabel_dict(ja_avg)\n",
    "zh_avg_relabeled = relabel_dict(zh_avg)\n",
    "\n",
    "df = pd.DataFrame([en_avg_relabeled, es_avg_relabeled, ja_avg_relabeled, zh_avg_relabeled],\n",
    "                   index=[\"English\", \"Spanish\", \"Japanese\", \"Chinese\"])\n",
    "\n",
    "labels = (df.T).applymap(lambda v: 'NA' if v == 0 else value_to_label_map[v])\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.figure(figsize=(14, 18))\n",
    "plt.title(\"Dialogue Acts: Average Act Importance & Act Frequency\", fontsize=24, pad=20)\n",
    "ax = sns.heatmap(df.T, annot=labels, \n",
    "                cmap=\"RdBu\", \n",
    "                vmin=-np.max(df.max()) , \n",
    "                vmax=np.max(df.max()), \n",
    "                annot_kws={\"size\": 18, 'verticalalignment': 'center', 'horizontalalignment': 'left'}, \n",
    "                cbar_kws={'aspect': 40, 'pad': 0.025},\n",
    "                linewidths=1, \n",
    "                fmt = '')\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.ax.tick_params(labelsize=16)\n",
    "plt.xticks(fontsize=22)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_keep = [\"fp\", \"sd\", \"sv\", \"ft\", \"qy\",  \"fc\"]\n",
    "\n",
    "en_avg_trimmed = {dialog_act_labels[x]: en_avg_raw[x] for x in keys_to_keep}\n",
    "es_avg_trimmed = {dialog_act_labels[x]: es_avg_raw[x] for x in keys_to_keep}\n",
    "ja_avg_trimmed = {dialog_act_labels[x]: ja_avg_raw[x] for x in keys_to_keep}\n",
    "zh_avg_trimmed = {dialog_act_labels[x]: zh_avg_raw[x] for x in keys_to_keep}\n",
    "\n",
    "en_counts_trimmed = {dialog_act_labels[x]: en_counts_raw[x] for x in keys_to_keep}\n",
    "es_counts_trimmed = {dialog_act_labels[x]: es_counts_raw[x] for x in keys_to_keep}\n",
    "ja_counts_trimmed = {dialog_act_labels[x]: ja_counts_raw[x] for x in keys_to_keep}\n",
    "zh_counts_trimmed = {dialog_act_labels[x]: zh_counts_raw[x] for x in keys_to_keep}\n",
    "\n",
    "value_to_label_map = {}\n",
    "for key in en_avg_trimmed.keys():\n",
    "    to_iterate = [en_avg_trimmed, es_avg_trimmed, ja_avg_trimmed, zh_avg_trimmed]\n",
    "    counts_to_iterate = [en_counts_trimmed, es_counts_trimmed, ja_counts_trimmed, zh_counts_trimmed]\n",
    "    sen_length_to_iterate = [num_sent_en, num_sent_es, num_sent_ja, num_sent_zh]\n",
    "    for idx, _ in enumerate(to_iterate):\n",
    "        val = to_iterate[idx][key]\n",
    "        count = counts_to_iterate[idx][key]\n",
    "        num_sent = sen_length_to_iterate[idx]\n",
    "        count_percent = count/num_sent * 100\n",
    "        #format val to string with 3 sig figs\n",
    "        val_string = \"{:.3f}\".format(val)\n",
    "        count_string = \"{:.1f}%\".format(count_percent)\n",
    "        label = val_string + \"  (\" + count_string + \")\"\n",
    "        value_to_label_map[val] = label\n",
    "\n",
    "en_avg_trimmed = relabel_dict(en_avg_trimmed)\n",
    "es_avg_trimmed = relabel_dict(es_avg_trimmed)\n",
    "ja_avg_trimmed = relabel_dict(ja_avg_trimmed)\n",
    "zh_avg_trimmed = relabel_dict(zh_avg_trimmed)\n",
    "\n",
    "df = pd.DataFrame([en_avg_trimmed, es_avg_trimmed, ja_avg_trimmed, zh_avg_trimmed],\n",
    "                     index=[\"English\", \"Spanish\", \"Japanese\", \"Chinese\"])\n",
    "\n",
    "labels = (df.T).applymap(lambda v: 'NA' if v == 0 else value_to_label_map[v])\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.title(\"Dialogue Acts: Average Act Importance & Act Frequency\", fontsize=24, pad=20)\n",
    "ax = sns.heatmap(df.T, annot=labels, \n",
    "                cmap=\"RdBu\", \n",
    "                vmin=-np.max(df.max()) , \n",
    "                vmax=np.max(df.max()), \n",
    "                annot_kws={\"size\": 18, 'verticalalignment': 'center', 'horizontalalignment': 'left'}, \n",
    "                cbar_kws={'aspect': 20, 'pad': 0.025},\n",
    "                linewidths=1, \n",
    "                fmt = '')\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.ax.tick_params(labelsize=16)\n",
    "plt.xticks(fontsize=22)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shreya_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
