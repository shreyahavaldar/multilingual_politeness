{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shreyah/.conda/envs/politeness/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import pickle \n",
    "from politenessr import Politenessr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = create_engine('mysql://127.0.0.1/wikipedia_talk?read_default_file=~/.my.cnf&charset=utf8')\n",
    "\n",
    "en_full = pd.read_sql('select * from msgs_en_turns', con)\n",
    "es_full = pd.read_sql('select * from msgs_es_turns', con)\n",
    "ja_full = pd.read_sql('select * from msgs_ja_turns', con)\n",
    "zh_full = pd.read_sql('select * from msgs_zh_turns', con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep only the message and language columns\n",
    "def process_turns(turns):\n",
    "    processed = []\n",
    "    for t in turns:\n",
    "        if(t == None): continue\n",
    "        if(len(t)<5): continue\n",
    "        t = t.replace('\\n', ' ')\n",
    "        t = t.replace('\\r', ' ')\n",
    "        t = t.replace('\\t', ' ')\n",
    "        t = t.replace('\"\"', '[temp filler for double quotes]')\n",
    "        t = t.replace('\"', '')\n",
    "        t = t.replace('[temp filler for double quotes]', '\"')\n",
    "        processed.append(t.strip())\n",
    "    return processed\n",
    "\n",
    "\n",
    "en_turns = process_turns(en_full['turn'])\n",
    "es_turns = process_turns(es_full['turn'])\n",
    "ja_turns = process_turns(ja_full['turn'])\n",
    "zh_turns = process_turns(zh_full['turn'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make them csvs\n",
    "en_trimmed = pd.DataFrame(en_turns, columns=['message'])\n",
    "en_trimmed.to_csv('full_data/en_trimmed.csv', index=True)\n",
    "\n",
    "es_trimmed = pd.DataFrame(es_turns, columns=['message'])\n",
    "es_trimmed.to_csv('full_data/es_trimmed.csv', index=True)\n",
    "\n",
    "ja_trimmed = pd.DataFrame(ja_turns, columns=['message'])\n",
    "ja_trimmed.to_csv('full_data/ja_trimmed.csv', index=True)\n",
    "\n",
    "zh_trimmed = pd.DataFrame(zh_turns, columns=['message'])\n",
    "zh_trimmed.to_csv('full_data/zh_trimmed.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06/13/2023 21:26:36 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /home/shreyah/.cache/torch/pytorch_transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\n",
      "06/13/2023 21:26:36 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "06/13/2023 21:26:36 - INFO - pytorch_transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /home/shreyah/.cache/torch/pytorch_transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n",
      "06/13/2023 21:26:42 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /home/shreyah/.pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "100%|██████████| 9/9 [00:00<00:00, 65.51it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.14s/it]\n"
     ]
    }
   ],
   "source": [
    "pr = Politenessr()\n",
    "en_labels = pr.predict(en_turns[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "from datasets import Dataset\n",
    "\n",
    "class MyPipeline(TextClassificationPipeline):   \n",
    "    def postprocess(self, model_outputs,return_all_scores=False):\n",
    "        print(model_outputs[\"logits\"][0])\n",
    "        score = model_outputs[\"logits\"][0]\n",
    "        return score.tolist()[0]\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"mpressi/english_xlm-False\")\n",
    "\n",
    "test = [\"I hate you\", \"I love you\"]\n",
    "\n",
    "# dataset = Dataset.from_list(test)\n",
    "\n",
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True, truncation=True)\n",
    "\n",
    "results = pipe(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.8569754362106323\n",
      "1.474205732345581\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[{'label': 'LABEL_0', 'score': 0.2857561409473419}],\n",
       " [{'label': 'LABEL_0', 'score': 0.8685514330863953}]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for r in results:\n",
    "    print(((r[0]['score'])-0.5) * 4)\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, return_all_scores=True)\n",
    "pipe(en_turns[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shreyah/.conda/envs/politeness/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "def read_pickle(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        pickle_file = pickle.load(file)\n",
    "    return pickle_file\n",
    "\n",
    "en_dialog = read_pickle(\"dialog_acts/dialog_act_data/english_dialog_acts_updated.p\")\n",
    "es_dialog = read_pickle(\"dialog_acts/dialog_act_data/spanish_dialog_acts_updated.p\")\n",
    "ja_dialog = read_pickle(\"dialog_acts/dialog_act_data/japanese_dialog_acts_updated.p\")\n",
    "zh_dialog = read_pickle(\"dialog_acts/dialog_act_data/chinese_dialog_acts_updated.p\")\n",
    "\n",
    "en_shap = read_pickle(\"shapley/xlm_shap_values/english.p\")\n",
    "es_shap = read_pickle(\"shapley/xlm_shap_values/spanish.p\")\n",
    "ja_shap = read_pickle(\"shapley/xlm_shap_values/japanese.p\")\n",
    "zh_shap = read_pickle(\"shapley/xlm_shap_values/chinese.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['' 'That ' 'is ' 'why ' 'he ' 'is ' 'a ' '‘' 'pre' 'tender' '’' '. '\n",
      " 'He ' 'has ' 'never ' 'claim' 'ed ' 'to ' 'be ' 'a ' 'King ' '- ' 'or '\n",
      " 'a ' 'Kaiser' ', ' 'for ' 'that ' 'matter' '. ' 'He ' 'is ' 'in ' 'the '\n",
      " 'same ' 'class ' 'as ' 'the ' 'Com' 'te ' 'de ' 'Paris' ', ' 'who ' 'is '\n",
      " 'not ' 'the ' 'King ' 'of ' 'France' ', ' 'but ' 'would ' 'be ' 'if '\n",
      " 'the ' 'Bour' 'bon' 's ' 'were ' 'placed ' 'on ' 'a ' 'resto' 'red '\n",
      " 'French ' 't' 'thro' 'ne' '.' '']\n",
      "['That is why he is a ‘pretender’.', 'He has never claimed to be a King - or a Kaiser, for that matter.', 'He is in the same class as the Comte de Paris, who is not the King of France, but would be if the Bourbons were placed on a restored French throne.']\n",
      "['' 'Tengo ' 'entend' 'ido ' 'que ' 'desde ' '1929' ', ' 'y ' 'sobre '\n",
      " 'todo ' 'desde ' '1948' ', ' 'la ' 'electro' 'di' 'ná' 'mica ' 'cu' 'án'\n",
      " 'tica ' 'de' 'ster' 'ró ' 'la ' 'teoría ' 'dual' '. ' 'Lo ' 'de ' 'la '\n",
      " 'teoría ' 'onda' '-' 'corp' 'ús' 'culo ' 'está ' 'histórica' 'mente '\n",
      " 'super' 'ado' ', ' 'aunque ' 'en ' 'algunos ' 'libros ' 'de ' 'bach'\n",
      " 'ille' 'rato ' 'todavía ' 'sea ' 'el ' 'no ' 'va ' 'más ' 'de ' 'la '\n",
      " 'Física' '.' '']\n",
      "['Tengo entendido que desde 1929, y sobre todo desde 1948, la electrodinámica cuántica desterró la teoría dual.', 'Lo de la teoría onda-corpúsculo está históricamente superado, aunque en algunos libros de bachillerato todavía sea el no va más de la Física.']\n",
      "['' '（' '日本の' '新聞' '一覧' 'は' '反対' '）' '上に' '同じ' '。 ' '日本' 'と' '海外' 'と' 'を'\n",
      " '区' '別' 'する' '必要' 'を感じ' 'ないので' '。' '']\n",
      "['（日本の新聞一覧は反対）上に同じ。', '日本と海外とを区別する必要を感じないので。']\n",
      "['' '請' '請問' '有關' '批評' '事宜' '是否有' '需要' '寫' '出' '或' '以' '中' '立' '角度' '寫'\n",
      " '出' '? ' '因為' '英文' '版' '也沒有' '列出' '相關' '事宜' '，' '外' '人' '一般' '較' '以' '理解'\n",
      " '。 ' '如果' '真的' '有' '需要' '寫' '出' '的話' '，' '有關' '以下' '言論' '是否' '需要' '以' '中'\n",
      " '立' '立場' '修改' '一下' '?' '']\n",
      "['請問有關批評事宜是否有需要寫出或以中立角度寫出?', '因為英文版也沒有列出相關事宜，外人一般較以理解。', '如果真的有需要寫出的話，有關以下言論是否需要以中立立場修改一下?']\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shreya_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
