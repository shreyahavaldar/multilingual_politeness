{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import generate_lexica\n",
    "import importlib\n",
    "importlib.reload(generate_lexica)\n",
    "import pickle\n",
    "import torch\n",
    "from transformers import XLMRobertaTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "SHAP_FLAG = \"XLM\"\n",
    "lexica_dict = generate_lexica.generate_lexica()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"\"\n",
    "if(SHAP_FLAG == \"XLM\"):\n",
    "    prefix = \"shapley/xlm_shap_values/\"\n",
    "    tokenizer_en = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
    "    tokenizer_es = tokenizer_en\n",
    "    tokenizer_ja = tokenizer_en\n",
    "    tokenizer_zh = tokenizer_en\n",
    "elif(SHAP_FLAG == \"MONO\"):\n",
    "    prefix = \"shapley/monolingual_shap_values/\"\n",
    "    tokenizer_en = AutoTokenizer.from_pretrained('roberta-base')\n",
    "    tokenizer_es = AutoTokenizer.from_pretrained('bertin-project/bertin-roberta-base-spanish')\n",
    "    tokenizer_ja = AutoTokenizer.from_pretrained('rinna/japanese-roberta-base', use_fast=False)\n",
    "    tokenizer_ja.do_lower_case = True\n",
    "    tokenizer_zh = AutoTokenizer.from_pretrained('hfl/chinese-roberta-wwm-ext')\n",
    "else:\n",
    "    raise Exception(\"SHAP_FLAG must be XLM or MONO\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_shap_values_pickle(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        shap_values = pickle.load(file)\n",
    "    return shap_values\n",
    "\n",
    "def get_token_level_shapley_values(shap_filepath, tokenizer, language):\n",
    "    shap_pickle = read_shap_values_pickle(shap_filepath)\n",
    "    token_level_shapley_values = []\n",
    "    utterances = []\n",
    "\n",
    "    shap_data = shap_pickle.data\n",
    "    shap_values = shap_pickle.values\n",
    "    shap_base = shap_pickle.base_values\n",
    "\n",
    "    for idx in tqdm(range(len(shap_data))):\n",
    "        tokens = shap_data[idx]\n",
    "        values = shap_values[idx]\n",
    "        base = shap_base[idx]\n",
    "        # divide base amongst all tokens\n",
    "        base_add = base/len(tokens)\n",
    "        values = [v + base_add for v in values]\n",
    "        \n",
    "        if(language == \"Spanish\" and SHAP_FLAG == \"MONO\"):\n",
    "            processed_tokens = []\n",
    "            #replace all accented characters in spanish with non-accented\n",
    "            to_replace = [\"á\", \"é\", \"í\", \"ó\", \"ú\", \"ü\", \"ñ\"]\n",
    "            replace_with = [\"a\", \"e\", \"i\", \"o\", \"u\", \"u\", \"n\"]\n",
    "            for token in tokens:\n",
    "                for i in range(len(to_replace)):\n",
    "                    token = token.replace(to_replace[i], replace_with[i])\n",
    "                processed_tokens.append(token)\n",
    "            tokens = processed_tokens\n",
    "        \n",
    "        utterance_string = tokenizer.convert_tokens_to_string(tokens)\n",
    "        \n",
    "        if(language == \"Chinese\" and SHAP_FLAG == \"MONO\"):\n",
    "            utterance_string = utterance_string.replace(\" \", \"\")\n",
    "        \n",
    "        utterances.append(utterance_string)\n",
    "        token_level_shapley_values.append([tokens, values, base])\n",
    "    return utterances, token_level_shapley_values\n",
    "\n",
    "def process_tokens(token_list):\n",
    "    #remove all spaces and punctuation from token that are added by tokenizer\n",
    "    to_replace = [\" \", \"_\", \"▁\", \".\", \"\\n\", \"Ġ\"]\n",
    "    #ensure utf-8 encoding\n",
    "    token_list = [x.encode('utf-8').decode('utf-8') for x in token_list]\n",
    "    for r in to_replace:\n",
    "        token_list = [x.replace(r, \"\") for x in token_list]\n",
    "    #make all lowercase\n",
    "    token_list = [str(x).lower() for x in token_list]\n",
    "    return token_list\n",
    "\n",
    "def get_start_end_idxs(shap_tokens, match_tokens):\n",
    "    start_idx = -1\n",
    "    end_idx = -1\n",
    "    for j in range(len(shap_tokens)):\n",
    "        if(shap_tokens[j] == match_tokens[0]):\n",
    "            if(shap_tokens[j:j+len(match_tokens)] == match_tokens):\n",
    "                start_idx = j\n",
    "                end_idx = j+len(match_tokens)\n",
    "    return start_idx, end_idx\n",
    "\n",
    "def get_start_end_idxs_contains(shap_tokens, match_tokens):\n",
    "    start_idx = -1\n",
    "    end_idx = -1\n",
    "    for j in range(len(shap_tokens)):\n",
    "        if(match_tokens[0] in shap_tokens[j]):\n",
    "            start_idx = j\n",
    "            end_idx = j+len(match_tokens)\n",
    "    return start_idx, end_idx\n",
    "\n",
    "def get_start_end_idxs_contains_reverse(shap_tokens, match_tokens):\n",
    "    start_idx = -1\n",
    "    end_idx = -1\n",
    "    match = \"\".join(match_tokens)\n",
    "    try:\n",
    "        for j in range(len(shap_tokens)):\n",
    "            word = \"\".join(shap_tokens[j:(j+len(match_tokens)+2)])\n",
    "            if(match in word):\n",
    "                start_idx = j\n",
    "                end_idx = j+len(match_tokens)+2\n",
    "            word = \"\".join(shap_tokens[j:(j+len(match_tokens)+1)])\n",
    "            match = \"\".join(match_tokens)\n",
    "            if(match in word):\n",
    "                start_idx = j\n",
    "                end_idx = j+len(match_tokens)+1    \n",
    "            word = \"\".join(shap_tokens[j:j+len(match_tokens)])\n",
    "            if(match in word):\n",
    "                start_idx = j\n",
    "                end_idx = j+len(match_tokens)  \n",
    "    except:\n",
    "        print(\"error finding idxs for %s in %s\" % (match_tokens, shap_tokens))\n",
    "    return start_idx, end_idx\n",
    "\n",
    "def get_all_strategies_token_level(language, detected_strategies, shap_object, tokenizer):  \n",
    "    assert(len(detected_strategies) == len(shap_object))\n",
    "    strategy_importances = { x:[] for x in lexica_dict[language].keys()}\n",
    "    strategy_counts = { x:0 for x in lexica_dict[language].keys()}\n",
    "    strategy_insights = { x:{} for x in lexica_dict[language].keys()}\n",
    "\n",
    "    internal_corr = { x:{} for x in lexica_dict[language].keys()}\n",
    "\n",
    "    for i, sentence_strategies in tqdm(enumerate(detected_strategies)):\n",
    "        for strat, matches in sentence_strategies.items():\n",
    "            for match in matches:\n",
    "                if(match == \"\"): continue\n",
    "                shap_tokens = process_tokens(shap_object[i][0])\n",
    "                shap_values = shap_object[i][1]\n",
    "                #process tokens\n",
    "                shap_tokens = [x.replace(\" \",\"\") for x in shap_tokens if x != \"\"]               \n",
    "                #get start and end index of where match is in shap_tokens\n",
    "                if(type(match) != str):\n",
    "                    match = match[1]\n",
    "                match_tokens = process_tokens(tokenizer.tokenize(match))\n",
    "                start_idx, end_idx = get_start_end_idxs(shap_tokens, match_tokens)\n",
    "                if(start_idx == -1 or end_idx == -1):\n",
    "                    if(language==\"English\" or language==\"Spanish\"):\n",
    "                        if(SHAP_FLAG == \"MONO\"):\n",
    "                            match_tokens = process_tokens(tokenizer.tokenize(\"  \" + match + \" \"))[1:-1]\n",
    "                        else:\n",
    "                            match_tokens = process_tokens(tokenizer.tokenize(\"(\" + match + \")\"))[1:-1]\n",
    "                        start_idx, end_idx = get_start_end_idxs(shap_tokens, match_tokens)\n",
    "                    else:\n",
    "                        start_idx, end_idx = get_start_end_idxs_contains(shap_tokens, match_tokens)\n",
    "                if(start_idx == -1 or end_idx == -1):\n",
    "                    if((language==\"English\" or language==\"Spanish\") and SHAP_FLAG == \"MONO\"):               \n",
    "                        match_tokens = process_tokens([\"\".join(tokenizer.tokenize(match))])\n",
    "                        start_idx, end_idx = get_start_end_idxs(shap_tokens, match_tokens)\n",
    "                if(start_idx == -1 or end_idx == -1):\n",
    "                    # print(\"no match for %s in %s\" % (match_tokens, shap_tokens))  \n",
    "                    continue\n",
    "\n",
    "                total_value = np.sum(shap_values[start_idx:end_idx])\n",
    "                strategy_importances[strat].append(total_value)\n",
    "                strategy_counts[strat] += 1\n",
    "                try:\n",
    "                    strategy_insights[strat][match.lower()].append(total_value)\n",
    "                    internal_corr[strat][match.lower()].append(np.sum(shap_values) + shap_object[i][2])\n",
    "                except:\n",
    "                    strategy_insights[strat][match.lower()] = []\n",
    "                    strategy_insights[strat][match.lower()].append(total_value)\n",
    "                    internal_corr[strat][match.lower()] = []\n",
    "                    internal_corr[strat][match.lower()].append(np.sum(shap_values) + shap_object[i][2])\n",
    "    return strategy_importances, strategy_counts, strategy_insights, internal_corr\n",
    "\n",
    "def detect_politeness_strategy(sentences, politeness_strategies):\n",
    "    count_no_strategies = 0\n",
    "    sentence_strategies = []\n",
    "    for sentence in sentences:\n",
    "        strategies = {}\n",
    "        for strategy, pattern in politeness_strategies.items():\n",
    "                if pattern.search(sentence):\n",
    "                    strategies[strategy] = pattern.findall(sentence)\n",
    "        sentence_strategies.append(strategies)\n",
    "        if(len(strategies) == 0):\n",
    "            count_no_strategies += 1\n",
    "    print(\"Number of sentences with no strategies: %d\" % count_no_strategies)\n",
    "    return sentence_strategies\n",
    "\n",
    "def process_strategy_results(strategies):\n",
    "    values = strategies[0]\n",
    "    counts = strategies[1]\n",
    "    values_avg = {}\n",
    "    if(\"please_start\" in values.keys()):\n",
    "        values[\"please\"] = [x for x in values[\"please\"] if x not in values[\"please_start\"]]\n",
    "        values[\"please\"] = [x for x in values[\"please\"] if x not in values[\"please_start\"]]\n",
    "    for key in values.keys():\n",
    "        if(counts[key] > 0):\n",
    "            avg = np.mean(values[key])\n",
    "            if(abs(avg) < 0.05): avg *= 2\n",
    "            values_avg[key] = avg\n",
    "    return values_avg\n",
    "\n",
    "def get_top_words(raw_strategies):\n",
    "    insights = raw_strategies[2]\n",
    "    insights_avg = {x:{} for x in insights.keys()}\n",
    "    for strat in insights.keys():\n",
    "        for match in insights[strat]:\n",
    "            count = len(insights[strat][match])\n",
    "            insights_avg[strat][match] = count\n",
    "        insights_avg[strat] = {k: v for k, v in sorted(insights_avg[strat].items(), key=lambda item: item[1], reverse=True)}\n",
    "    top_words = {}\n",
    "    for strat in insights_avg.keys():\n",
    "        top_words[strat] = list(insights_avg[strat].keys())\n",
    "    return top_words\n",
    "\n",
    "def relabel_dict(dictionary, top_words=None):\n",
    "    #rename keys to be more readable\n",
    "    new_dict = {}\n",
    "    for key in dictionary.keys():\n",
    "        key_new = key.replace(\"_\", \" \")\n",
    "        key_new = key_new.replace(\"first\", \"1st\")\n",
    "        key_new = key_new.replace(\"ident\", \"identity\")\n",
    "        key_new = key_new.replace(\"1st person singular\", \"1st person\")\n",
    "        key_new = key_new.replace(\"second\", \"2nd\")\n",
    "        key_new = key_new.replace(\"hedge\", \"hedges\")\n",
    "        key_new = key_new.replace(\"btw\", \"(btw)\")\n",
    "        key_new = key_new.replace(\"has \", \"\")\n",
    "        key_new = key_new.replace(\"you direct\", \"direct \\\"You\\\"\")\n",
    "        key_new = key_new.replace(\"you honorific\", \"honorific \\\"You\\\"\")\n",
    "        key_new = \" \".join([x[0].upper() + x[1:] for x in key_new.split(\" \")])\n",
    "\n",
    "        if(top_words != None):\n",
    "            key_new += (\"\\n(%s)\" % top_words[key])\n",
    "            \n",
    "        new_dict[key_new] = dictionary[key]\n",
    "    return new_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Strategies across Utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utterances_en, shap_en = get_token_level_shapley_values(prefix+\"english.p\", tokenizer_en, \"English\")\n",
    "detected_strategies_en = detect_politeness_strategy(utterances_en, lexica_dict[\"English\"])\n",
    "strategies_english_raw = get_all_strategies_token_level(\"English\", detected_strategies_en, shap_en, tokenizer_en)\n",
    "\n",
    "utterances_es, shap_es = get_token_level_shapley_values(prefix+\"spanish.p\", tokenizer_es, \"Spanish\")\n",
    "detected_strategies_es = detect_politeness_strategy(utterances_es, lexica_dict[\"Spanish\"])\n",
    "strategies_spanish_raw = get_all_strategies_token_level(\"Spanish\", detected_strategies_es, shap_es, tokenizer_es)\n",
    "\n",
    "utterances_ja, shap_ja = get_token_level_shapley_values(prefix+\"japanese.p\", tokenizer_ja, \"Japanese\")\n",
    "detected_strategies_ja = detect_politeness_strategy(utterances_ja, lexica_dict[\"Japanese\"])\n",
    "strategies_japanese_raw = get_all_strategies_token_level(\"Japanese\", detected_strategies_ja, shap_ja, tokenizer_ja)\n",
    "\n",
    "utterances_zh, shap_zh = get_token_level_shapley_values(prefix+\"chinese.p\", tokenizer_zh, \"Chinese\")\n",
    "detected_strategies_zh = detect_politeness_strategy(utterances_zh, lexica_dict[\"Chinese\"])\n",
    "strategies_chinese_raw = get_all_strategies_token_level(\"Chinese\", detected_strategies_zh, shap_zh, tokenizer_zh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies_english = process_strategy_results(strategies_english_raw)\n",
    "strategies_spanish = process_strategy_results(strategies_spanish_raw)\n",
    "strategies_japanese = process_strategy_results(strategies_japanese_raw)\n",
    "strategies_chinese = process_strategy_results(strategies_chinese_raw)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: All Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sent_en =  15624\n",
    "num_sent_es =  14952\n",
    "num_sent_ja =  13057\n",
    "num_sent_zh =  13656\n",
    "\n",
    "en_values = strategies_english.copy()\n",
    "es_values = strategies_spanish.copy()\n",
    "zh_values = strategies_chinese.copy()\n",
    "ja_values = strategies_japanese.copy()\n",
    "\n",
    "en_counts = strategies_english_raw[1]\n",
    "es_counts = strategies_spanish_raw[1]\n",
    "zh_counts = strategies_chinese_raw[1]\n",
    "ja_counts = strategies_japanese_raw[1]\n",
    "\n",
    "all_keys = set(list(en_values.keys()) + list(es_values.keys()) + list(zh_values.keys()) + list(ja_values.keys()))\n",
    "for key in all_keys:\n",
    "    if(key not in en_values.keys()):\n",
    "        en_values[key] = 0\n",
    "        en_counts[key] = 0\n",
    "    if(key not in es_values.keys()):\n",
    "        es_values[key] = 0\n",
    "        es_counts[key] = 0\n",
    "    if(key not in zh_values.keys()):\n",
    "        zh_values[key] = 0\n",
    "        zh_counts[key] = 0\n",
    "    if(key not in ja_values.keys()):\n",
    "        ja_values[key] = 0\n",
    "        ja_counts[key] = 0\n",
    "\n",
    "\n",
    "value_to_label_map = {}\n",
    "for key in en_values.keys():\n",
    "    to_iterate = [en_values, es_values, ja_values, zh_values]\n",
    "    counts_to_iterate = [en_counts, es_counts, ja_counts, zh_counts]\n",
    "    sen_length_to_iterate = [num_sent_en, num_sent_es, num_sent_ja, num_sent_zh]\n",
    "    for idx, _ in enumerate(to_iterate):\n",
    "        val = to_iterate[idx][key]\n",
    "        count = counts_to_iterate[idx][key]\n",
    "        num_sent = sen_length_to_iterate[idx]\n",
    "        count_percent = count/num_sent * 100\n",
    "        #format val to string with 3 sig figs\n",
    "        val_string = \"{:.3f}\".format(val)\n",
    "        count_string = \"{:.1f}%\".format(count_percent)\n",
    "        label = val_string + \"  (\" + count_string + \")\"\n",
    "        value_to_label_map[val] = label\n",
    "\n",
    "en_values_relabeled = relabel_dict(en_values)\n",
    "es_values_relabeled = relabel_dict(es_values)\n",
    "ja_values_relabeled = relabel_dict(ja_values)\n",
    "zh_values_relabeled = relabel_dict(zh_values)\n",
    "        \n",
    "df = pd.DataFrame([en_values_relabeled, es_values_relabeled, ja_values_relabeled, zh_values_relabeled],\n",
    "                   index=[\"English\", \"Spanish\", \"Japanese\", \"Chinese\"])\n",
    "\n",
    "labels = (df.T).applymap(lambda v: 'NA' if v == 0 else value_to_label_map[v])\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.figure(figsize=(14, 20))\n",
    "plt.title(\"PoliteLex: Average Category Importance & Category Frequency\", fontsize=24, pad=20)\n",
    "ax = sns.heatmap(df.T, annot=labels, \n",
    "                cmap=\"RdBu\", \n",
    "                vmin=-np.max(df.max()) , \n",
    "                vmax=np.max(df.max()), \n",
    "                annot_kws={\"size\": 18, 'verticalalignment': 'center', 'horizontalalignment': 'left'}, \n",
    "                cbar_kws={'aspect': 40, 'pad': 0.025},\n",
    "                linewidths=1, \n",
    "                fmt = '')\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.ax.tick_params(labelsize=16)\n",
    "plt.xticks(fontsize=22)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization: Selected Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_keep = [\"please\", \"gratitude\", \"apologetic\", \"greeting\", \"emergency\", \"ingroup_ident\", \"you_direct\", \"you_honorific\", \"taboo\"]\n",
    "\n",
    "en_values_trimmed = {k: v for k, v in en_values.items() if k in keys_to_keep}\n",
    "es_values_trimmed = {k: v for k, v in es_values.items() if k in keys_to_keep}\n",
    "ja_values_trimmed = {k: v for k, v in ja_values.items() if k in keys_to_keep}\n",
    "zh_values_trimmed = {k: v for k, v in zh_values.items() if k in keys_to_keep}\n",
    "\n",
    "en_counts_trimmed = {k: v for k, v in en_counts.items() if k in keys_to_keep}\n",
    "es_counts_trimmed = {k: v for k, v in es_counts.items() if k in keys_to_keep}\n",
    "ja_counts_trimmed = {k: v for k, v in ja_counts.items() if k in keys_to_keep}\n",
    "zh_counts_trimmed = {k: v for k, v in zh_counts.items() if k in keys_to_keep}\n",
    "\n",
    "\n",
    "value_to_label_map = {}\n",
    "for key in keys_to_keep:\n",
    "    to_iterate = [en_values_trimmed, es_values_trimmed, ja_values_trimmed, zh_values_trimmed]\n",
    "    counts_to_iterate = [en_counts_trimmed, es_counts_trimmed, ja_counts_trimmed, zh_counts_trimmed]\n",
    "    sen_length_to_iterate = [num_sent_en, num_sent_es, num_sent_ja, num_sent_zh]\n",
    "    for idx, _ in enumerate(to_iterate):\n",
    "        val = to_iterate[idx][key]\n",
    "        count = counts_to_iterate[idx][key]\n",
    "        num_sent = sen_length_to_iterate[idx]\n",
    "        count_percent = count/num_sent * 100\n",
    "        #format val to string with 3 sig figs\n",
    "        val_string = \"{:.3f}\".format(val)\n",
    "        count_string = \"{:.1f}%\".format(count_percent)\n",
    "        label = val_string + \"  (\" + count_string + \")\"\n",
    "        value_to_label_map[val] = label\n",
    "\n",
    "en_values_trimmed = relabel_dict(en_values_trimmed)\n",
    "es_values_trimmed = relabel_dict(es_values_trimmed)\n",
    "zh_values_trimmed = relabel_dict(zh_values_trimmed)\n",
    "ja_values_trimmed = relabel_dict(ja_values_trimmed)\n",
    "\n",
    "df = pd.DataFrame([en_values_trimmed, es_values_trimmed, ja_values_trimmed, zh_values_trimmed], \n",
    "                   index=[\"English\", \"Spanish\", \"Japanese\", \"Chinese\"])\n",
    "\n",
    "labels = (df.T).applymap(lambda v: 'NA' if v == 0 else value_to_label_map[v])\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.title(\"PoliteLex: Average Category Importance & Category Frequency\", fontsize=24, pad=20)\n",
    "ax = sns.heatmap(df.T, annot=labels, \n",
    "                cmap=\"RdBu\", \n",
    "                vmin=-np.max(df.max()) , \n",
    "                vmax=np.max(df.max()), \n",
    "                annot_kws={\"size\": 18, 'verticalalignment': 'center', 'horizontalalignment': 'left'}, \n",
    "                cbar_kws={'aspect': 25, 'pad': 0.025},\n",
    "                linewidths=1, \n",
    "                fmt = '')\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.ax.tick_params(labelsize=16)\n",
    "plt.xticks(fontsize=22)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get average of sentence lengths for all languages\n",
    "en_lengths_avg = np.average([len(x) for x in utterances_en])\n",
    "es_lengths_avg = np.average([len(x) for x in utterances_es])\n",
    "zh_lengths_avg = np.average([len(x) for x in utterances_zh])\n",
    "ja_lengths_avg = np.average([len(x) for x in utterances_ja])\n",
    "\n",
    "#print all\n",
    "print(\"Sentence Lengths:\")\n",
    "print(\"English: \" + str(en_lengths_avg))\n",
    "print(\"Spanish: \" + str(es_lengths_avg))\n",
    "print(\"Chinese: \" + str(zh_lengths_avg))\n",
    "print(\"Japanese: \" + str(ja_lengths_avg))\n",
    "\n",
    "#load pickle files\n",
    "shap_en_test = shap_pickle = read_shap_values_pickle(prefix+\"english.p\")\n",
    "shap_es_test = shap_pickle = read_shap_values_pickle(prefix+\"spanish.p\")\n",
    "shap_ja_test = shap_pickle = read_shap_values_pickle(prefix+\"japanese.p\")\n",
    "shap_zh_test = shap_pickle = read_shap_values_pickle(prefix+\"chinese.p\")\n",
    "\n",
    "#get average base values\n",
    "base_en_avg = np.average(shap_en_test.base_values)\n",
    "base_es_avg = np.average(shap_es_test.base_values)\n",
    "base_ja_avg = np.average(shap_ja_test.base_values)\n",
    "base_zh_avg = np.average(shap_zh_test.base_values)\n",
    "\n",
    "#get average values\n",
    "shap_en_avg = np.average([np.sum(x) for x in shap_en_test.values])\n",
    "shap_es_avg = np.average([np.sum(x) for x in shap_es_test.values])\n",
    "shap_ja_avg = np.average([np.sum(x) for x in shap_ja_test.values])\n",
    "shap_zh_avg = np.average([np.sum(x) for x in shap_zh_test.values])\n",
    "\n",
    "#get min and max values\n",
    "shap_en_max = np.max([np.sum(x) for x in shap_en_test.values])\n",
    "shap_es_max = np.max([np.sum(x) for x in shap_es_test.values])\n",
    "shap_ja_max = np.max([np.sum(x) for x in shap_ja_test.values])\n",
    "shap_zh_max = np.max([np.sum(x) for x in shap_zh_test.values])\n",
    "\n",
    "shap_en_min = np.min([np.sum(x) for x in shap_en_test.values])\n",
    "shap_es_min = np.min([np.sum(x) for x in shap_es_test.values])\n",
    "shap_ja_min = np.min([np.sum(x) for x in shap_ja_test.values])\n",
    "shap_zh_min = np.min([np.sum(x) for x in shap_zh_test.values])\n",
    "\n",
    "#print all\n",
    "print(\"\\nAverage Base Value and Average Shapley Value Sums:\")\n",
    "print(\"English: avg base_val: \" + str(base_en_avg) + \", avg shap_val: \" + str(shap_en_avg) + \", avg sum: \" + str(base_en_avg + shap_en_avg))\n",
    "print(\"Spanish: avg base_val: \" + str(base_es_avg) + \", avg shap_val: \" + str(shap_es_avg) + \", avg sum: \" + str(base_es_avg + shap_es_avg))\n",
    "print(\"Chinese: avg base_val: \" + str(base_zh_avg) + \", avg shap_val: \" + str(shap_zh_avg) + \", avg sum: \" + str(base_zh_avg + shap_zh_avg))\n",
    "print(\"Japanese: avg base_val: \" + str(base_ja_avg) + \", avg shap_val: \" + str(shap_ja_avg) + \", avg sum: \" + str(base_ja_avg + shap_ja_avg))\n",
    "\n",
    "#print max and min\n",
    "print(\"\\nMax and Min Shapley Values:\")\n",
    "print(\"English: \" + \"max: \" + str(shap_en_max) + \", min: \" + str(shap_en_min))\n",
    "print(\"Spanish: \" + \"max: \" + str(shap_es_max) + \", min: \" + str(shap_es_min))\n",
    "print(\"Chinese: \" + \"max: \" + str(shap_zh_max) + \", min: \" + str(shap_zh_min))\n",
    "print(\"Japanese: \" + \"max: \" + str(shap_ja_max) + \", min: \" + str(shap_ja_min))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights into Detected Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_insights(raw_strategies):\n",
    "    #get average shap val of each word in each strategy\n",
    "    insights = raw_strategies[2]\n",
    "    insights_avg = {x:{} for x in insights.keys()}\n",
    "    for strat in insights.keys():\n",
    "        print(\"\\n-----\" + strat.upper() + \"-----\")\n",
    "        for match in insights[strat]:\n",
    "            avg = np.average(insights[strat][match])\n",
    "            insights_avg[strat][match] = avg\n",
    "        #sort insights_avg by count\n",
    "        insights_avg[strat] = {k: v for k, v in sorted(insights_avg[strat].items(), key=lambda item: -len(insights[strat][item[0]]))}\n",
    "        for match in insights_avg[strat]:\n",
    "            #round median and avg to 5 dec pts\n",
    "            median = round(np.median(insights[strat][match]), 5)\n",
    "            avg = round(np.average(insights[strat][match]), 5)\n",
    "            print(match + \": \" + str(avg) + \", median: \" + str(median) + \", count: \" + str(len(insights[strat][match])))\n",
    "\n",
    "top_en = get_top_words(strategies_english_raw)\n",
    "top_en = relabel_dict(top_en)\n",
    "\n",
    "for key in top_en.keys():\n",
    "    top_3 = top_en[key][:3]\n",
    "    print(key + \" & \" + \"\\\\texttt{\" + \", \".join(top_3) + \"}\"  +\" \\\\\\\\\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shreya_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
